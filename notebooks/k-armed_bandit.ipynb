{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "import RLGlue\n",
    "sys.path.append('../scripts/')\n",
    "import main_agent\n",
    "import ten_arm_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        # return a random selection from ties.\n",
    "\n",
    "        if i == 0:\n",
    "            top = q_values[i]\n",
    "            # list stores index of max\n",
    "            ties = [i]\n",
    "        else:\n",
    "            if q_values[i] > q_values[ties[0]]:\n",
    "                top = q_values[i]\n",
    "                ties = [i]\n",
    "            elif q_values[i] == q_values[ties[0]]:\n",
    "                ties.append(i)\n",
    "        #print(ties)\n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent(main_agent.Agent):\n",
    "    def agent_step(self, reward, observation=None):\n",
    "        \"\"\"\n",
    "        Takes one step for the agent. It takes in a reward and observation and \n",
    "        returns the action the agent chooses at that time step.\n",
    "        \n",
    "        Arguments:\n",
    "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
    "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
    "                              until future lessons\n",
    "        Returns:\n",
    "        current_action -- int, the action chosen by the agent at the current time step.\n",
    "        \"\"\"\n",
    "        ### Useful Class Variables ###\n",
    "        # self.q_values : An array with what the agent believes each of the values of the arm are.\n",
    "        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n",
    "        # self.last_action : The action that the agent took on the previous time step\n",
    "        #######################\n",
    "        \n",
    "        # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.\n",
    "        # increment the counter in self.arm_count for the action from the previous time step\n",
    "        # update the step size using self.arm_count\n",
    "        # update self.q_values for the action from the previous time step\n",
    "        \n",
    "        self.arm_count[self.last_action] += 1\n",
    "        step_size = 1 / self.arm_count[self.last_action]\n",
    "        self.q_values[self.last_action] = self.q_values[self.last_action] + step_size * (reward - self.q_values[self.last_action])\n",
    "        \n",
    "        current_action = argmax(self.q_values)\n",
    "    \n",
    "        self.last_action = current_action\n",
    "        \n",
    "        return current_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb58c0f0287f3c8b01c0158c68142f01d63e77dd0edcf5fc06b028e2b1a9d2c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rl_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
