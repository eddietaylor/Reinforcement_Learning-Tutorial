{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-When we think about the nature of learning, we think about interaction\n",
    "\n",
    "-Reinforcement learning focuses on a computational approach to goal-directed learning from interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The learner is not specifically told which actions to take, but learns through trial and error in addition to delayed reward\n",
    "  - these two features distinguish RL from other ML\n",
    "  \n",
    "-Formally, RL uses ideas from dynamical systems theory, specifically, \"incompletely-known Markov decision processes\"\n",
    "  - learning agent must sense environment, must be able to take action and must have goals\n",
    "\n",
    "-RL contrasts with supervised learning in that SL seeks to obtain generalizability from historical labels\n",
    "  - RL meanwhile is trying to learn from *interaction* and not necessarily from charted territory\n",
    "\n",
    "-unsupervised learning is also distinguished by the fact that its trying to find representations in the data and it has nothing to do with a reward signal\n",
    "\n",
    "-Therefore, we consider RL to be a third paradigm of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-One of the challenges of RL is the trade-off between exploration and exploitation\n",
    "\n",
    "  - to maximize reward, it can exploit past behaviors its deemed rewarding\n",
    "  - but to find those behaviors in the first place, it has to explore and possibly lose reward\n",
    "  - The catch is that the task cannot be accomplished by either one exclusively\n",
    "  - The exploration-exploitation dilemna remains unsolved\n",
    "\n",
    "-RL is part of a swing of the pendulum in AI research towards discovery of principles, as opposed to rules based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Elements of Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The 4 main elements of an RL system are: a *policy*, a *reward signal*, a *value function* and optionally, a *model* of the environment\n",
    "\n",
    "  - a **policy** defines the agent's way of behaving. \n",
    "    - Its a mapping from perceived states of the environment to actions to be taken in those states (in psychology, called *associations* and from their perspective is the basis for learning)\n",
    "    - may be as simple as a lookup or as complicated as a search process\n",
    "\n",
    "  - a **reward signal** defines the goal of the RL system. \n",
    "    - The reward is that which the system is trying to maximize and is the primary basis for altering policy\n",
    "    - it is more immediate term\n",
    "\n",
    "  - a **value function** specifies what is good in the long-term\n",
    "    - The specifies the total amount of reward which can be expected in the long-term\n",
    "\n",
    "  - But rewards are primary, and values are predictions of rewards. **Without rewards there could be no values.**\n",
    "    - However, when making and evaluating decision, we use values since they provide the most reward to us over time\n",
    "    - Values are difficult to determine, while rewards are a given\n",
    "    - Value estimation is one of the most important things to have been progressed in RL research in the last 60 years\n",
    "\n",
    "  - a **model** of the environment allows inferences to be made about how the environment will behave\n",
    "    - e.g. given a state and an action, the model might predict next state and reward\n",
    "    - models are used for planning and are not in all RL systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Limitations and Scope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-RL relies on the concept of \"state\", as input to the policy and value function\n",
    "  - essentially it conveys to the agent some sense of how the environment is\n",
    "  - We will focus mainly on estimating value functions (but you don't need this to solve RL problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 An Extetnded Example: Tic-Tac-Toe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-assuming you play against an imperfect player, how can we construct a player which will learn the opponent's imperfections and maximize its chance of winning?\n",
    "\n",
    "  - classicial optimization techniques (dynamic programming) require complete specification of opponent, including the probabilities with which the opponent makes each move\n",
    "\n",
    "  - essentially, we can learn a model of the opponent's behavior from experience to get these probabilities and then apply dynamic programming (many RL methods do this)\n",
    "\n",
    "  - or, using a value function. \n",
    "    - We would set of a table of numbers, each representing the probabilities of winning from each state of the tic-tac-toe board\n",
    "    - this estimate is the state's value\n",
    "    - set all initial values of the states to 0.5\n",
    "    - then play many games against the opponent, greedily selecting the moves (moving to states with greatest value) and occasionally randomly selecting another move to explore\n",
    "    - then we \"back up\" value of states so that earlier states are nudged closer in value to later states\n",
    "\n",
    "$$ V(S_t) \\larr V(S_t) + \\alpha[V(S_{t+1}) - V(S_t)] $$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/tictactoe.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-tic tac toe example is just one\n",
    "  - with RL problems, we don't necessarily need an adversary\n",
    "  - we don't necessarily need discrete time\n",
    "  - we don't necessarily need a finite state space like in tic-tac-toe (in infinite state space games, we might utilize the generalizability of SL)\n",
    "  - it can work even when certain states are hidden\n",
    "  - with tic-tac-toe we were able to look ahead and know the states that would come from moves (this is a model of the game). Even this is not necessary for RL. \n",
    "  - but with respect to opponent, our tic-tac-toe player actually has no model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.6 Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-RL is a computational approach to understanding and automating goal-directed learning and decision making\n",
    "  - it distinguishes itself from other computational approaches in its interaction-based learning and the lack of need of direct supervision or model of environment\n",
    "  - its fundamentally based on Markov decision processes\n",
    "  - the concepts of value and value function are key to most RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.7 History of RL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Early History of RL contains two threads which eventually merged:\n",
    "  - Psychology of animal learning\n",
    "  - Optimal control and dynamic programming\n",
    "\n",
    "-They merged around a third thread known as temporal difference methods and modern RL began in 80s\n",
    "\n",
    "-Optimal control thread centers around Bellman equation and the class of methods used to solve this equation known as dynamic programming\n",
    "  - Bellman also introduced the discrete stochastic version of this problem known as Markov decision processes\n",
    "\n",
    "-trial-and-error learning was first coherently captured b Edward Thorndike\n",
    "  - The \"Law of Effect\": Behaviors which produce a satisfying effect are more likely to occur when the situation presents itself again and behaviors which produce a painful effect are less likely to occur for that situation\n",
    "\n",
    "  - Later learning automata had an influence on trial-and-error learning (k-armed bandit)\n",
    "\n",
    "-temporal difference learning has its origins in animal psychology with *secondary reinforcers*\n",
    "  - It was brought together with optimal control with Chris Watkin's Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-armed Bandits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The most important difference between RL and SL is that RL *evaluates* the actions of the bot rather than *instructing*\n",
    "  - SL indicates the correct action to be taken, independent of the action\n",
    "  - RL indicates how good the action was\n",
    "\n",
    "-To begin, we study the evaluative aspect of RL in a simplified setting which does not involve learning to act in more than one situation (*nonassociative setting*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 A *k*-armed Bandit Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-You are faced repeatedly with a choice among *k* different options, or actions\n",
    "\n",
    "  - After each choice, you receive a numerical reward chosen from a stationary PDF that depends on the action you selected\n",
    "  - Your objective is to maximize the expected total reward over some time period, for example 1000 time steps\n",
    "\n",
    "- This problem is named after the analogy to the slot machine (the \"one-armed bandit\"), except that it has k levers\n",
    "\n",
    "-Each of the *k* actions has an expected reward, and we call this the *value* of the action:\n",
    "\n",
    "$$ q_{*}(a) \\coloneqq \\mathbf{E}[R_{t}|A_{t}=a] $$\n",
    "\n",
    "Where $q_{*}(a)$ is the expected reqward, given that action $a$ is selected. $R_{t}$ is the reward and $A_{t}$ is the action\n",
    "\n",
    "-To estimate this, we would need a joint distribution of $R_{t}$ and $A_{t}$\n",
    "\n",
    "-If you maintain estimates of the value of actions, then at any time step, there is at least one action whose estimated value is the greatest\n",
    "  - Taking this action would be the *greedy* action and when you do, we say that you are *exploiting* you current knowledge of the values of the actions\n",
    "  - If you choice one of the nongreedy actions, we say that you are *exploring* (this might lead to a greater reward in the long term)\n",
    "  - Balancing exploitation and exploration is a difficult task but leads to a more optimal solution than just exploiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Action-value Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-estimate the values of actions and use the estimates to make action selection decisions\n",
    "\n",
    "-Simply average the rewards seen when particular action was taken:\n",
    "\n",
    "$$ Q_{t}(a) \\coloneqq \\frac{\\text{sum of rewards when $a$ taken prior to t}}{\\text{number of times $a$ taken prior to t}} $$\n",
    "\n",
    "$$ Q_{t}(a) = \\frac{\\sum_{i=1}^{t-1}R_{i}\\cdot \\bm{1}_{A_i=a}}{\\sum_{i=1}^{t-1} \\bm{1}_{A_{i}=a}} $$\n",
    "\n",
    "as t goes to infinity, $Q_{t}(a)$ converges to $q_{*}(a)$.\n",
    "\n",
    "-This is not necessarily the best way to estimate the value! \n",
    "\n",
    "-Simplest action selection rule is to select the action with the highest estimated value (greedy):\n",
    "\n",
    "$$A_{t} \\coloneqq \\underset{a}{\\mathrm{argmax}}Q_{t}(a) $$\n",
    "\n",
    "-Slight mod of this is behaving greedy most of the time, but every once in a while, with small probability $\\epsilon$, select randomly from all actions\n",
    "\n",
    "  - we call these methods $\\epsilon-greedy$ methods\n",
    "  - advantage here is that in the limit of $t \\rightarrow \\infty$, you sample all of the actions infinitely thus ensuring  $Q_{t}(a)$ converges to $q_{*}(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 The 10-armed Testbed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To assess greedy vs $\\epsilon-greedy$, we analyze a set of 2000 randomly generated $k$-armed bandit problems with $k=10$\n",
    "\n",
    "-The true values, $q_{*}(a)$, were selected from a normal distribution. The reward, $R_{t}, for each action is also a normal distribution but with mean $q_{*}(A_{t})$\n",
    "\n",
    "-For any learning method, we can measure its performance for 1000 steps, and this constitutes 1 run\n",
    "  - then repeat this procedure 2000 times (effectively a different bandit problem each time)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/10armedtestbed.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-What you then see is that the greedy algorithm performs better initially but plateaus at a lower reward\n",
    "\n",
    "-The slight exploration allows discovery of that higher expected reward of 1.5\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/10armedlearningcurves.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-For noisier rewards (more variance), the $\\epsilon-greedy$ methods would fare even better than the greedy model\n",
    "\n",
    "-In the deterministic case of no variance, the greedy would quickly find the solution\n",
    "  - But if the values of the actions were not stationary, it would be worthwhile to explore to make sure one of the other actions have not changed to become the better one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Incremental Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To calculate action values in a computationally efficient manner, we will consider a simple derivation\n",
    "\n",
    "-Let $R_{i}$ denote the reward received after the $ith$ selection of *this action* and let $Q_{n}$ denote the estimate of its action value after having been selected $n-1$ time:\n",
    "\n",
    "$$ Q_{n} \\coloneqq \\frac{R_{1} + R_{2} + ... + R_{n-1}}{n-1} $$\n",
    "\n",
    "-We don't need to perform this computation over and over again\n",
    "\n",
    "-It can be shown that, given $Q_{n}$ and the $nth$ reward $R_{n}$:\n",
    "\n",
    "$$ Q_{n+1} = Q_{n} + \\frac{1}{n}[R_{n} - Q_{n}] $$\n",
    "\n",
    "so you only need memory for $Q_{n}$ and $n$\n",
    "\n",
    "-This update rule thats the form that comes up frequently:\n",
    "\n",
    "$$ \\text{New Estimate} \\leftarrow \\text{Old Estimate} + \\text{Step Size}[\\text{Target - Old Estimate}] $$\n",
    "\n",
    "-In this case the target is the nth reward and $\\text{Target - Old Estimate}$ is an error estimate\n",
    "\n",
    "-Note that the $\\text{Step Size}$ gets smaller with increasing actions of this type taken\n",
    "\n",
    "-Below is the pseudo code for the bandit algorithm\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/banditpseudo.png\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Tracking a  Nonstationary Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-When reward probabilities change over time, it makes sense to give more weight to recent rewards\n",
    "\n",
    "-It's common to modify the step size parameter to be constant for nonstationary reward pdfs\n",
    "\n",
    "-It can be shown that in this case, $Q_{n+1}$ becomes a weighted average of the past rewards:\n",
    "\n",
    "$$Q_{n+1} = (1 - \\alpha)^{n}Q_{1} + \\sum_{i=1}^{n}\\alpha(1-\\alpha)^{n-i}R_{i} $$\n",
    "\n",
    "-This turns out to be an exponentially weighted average!\n",
    "\n",
    "-For the constant $\\alpha$ case, the value function estimates actually never converge for large $n$, but this is what we want in nonstationary problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finite Markov Decision Processes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-This essentially the main problem we try to solve\n",
    "\n",
    "-There is evaluative feedback, as with the bandits, but now there is an associative aspect (choosing different actions in different situations)\n",
    "  - Actions influence not just immediate rewards, but also subsequent situations (or states)\n",
    "  - Hence MDPs involve delayed reward and the need to tradeoff immediate and delayed reward\n",
    "\n",
    "-With k-armed bandits, we estimated $q_{*}(a)$ of each action $a$, while with MDPs, we estimate $q_{*}(s,a)$ or each action $a$ in each state $s$\n",
    "  - or we estimate the value $v_{*}(s)$ of each state given optimal action selections\n",
    "\n",
    "-MDPs are a mathematically idealized form of the RL problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 The Agent-Environment Interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The learner and decision maker is called the *agent*\n",
    "\n",
    "-The thing it interacts with is called the *environment*\n",
    "\n",
    "-The agent and environment interact continually with agent selecting actions and environment responding to these actions by presenting new situations\n",
    "\n",
    "  - The environment also gives rise to rewards, which the agent seeks to maximize over time with its actions\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/MDP.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The agent and environment interact at discrete time steps $t=0,1,2,3,...$\n",
    "\n",
    "-At each time step, the agent receives a representation of the environment's state, $S_t \\in S$ and from that, selects an action, $A_t \\in A(s)$\n",
    "\n",
    "  - One time step later, the agent receives a reward $R_{t+1} \\in R$ and finds itself in a new state $S_{t+1}$\n",
    "\n",
    "  - The trajectory looks like this:\n",
    "\n",
    "  $$ S_0, A_0, R_0, S_1, A_1, R_1, S_2, A_2, R_2, ... $$\n",
    "\n",
    "-In a *finite* MDP, the sets of states, actions, and rewards ($S, A, R$) all have a finite number of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In this case, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions dependent only on the preceding state and action:\n",
    "\n",
    "$$ p(s',r|s,a) \\coloneqq Pr(S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In a *Markov* decision process, the probabilities given by $p$ completely characterize the environment's dynamics\n",
    "\n",
    "  - The probability of each $S_t$ and $R_t$ only depend on the immediately preceding state and action and not at all on earlier states and actions\n",
    "\n",
    "  - The state must include all aspects of the past agent-environment that make a difference for the future (*Markov property*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Can compute the state-transition probabilities:\n",
    "\n",
    "$$ p(s'|s,a) \\coloneqq Pr(S_t=s'|S_{t-1}=s,A_{t-1}=a) = \\sum_{r \\in R}p(s',r|s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Can compute the expected rewards for state-action pairs\n",
    "\n",
    "$$ r(s,a) \\coloneqq \\mathbf{E}[R_{t}|S_{t-1}=s,A_{t-1}=a] = \\sum_{r \\in R}r\\sum_{s' \\in S}p(s',r|s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
